![](./assets/sophgo_chip.png)

<p align="center">
    <a href=""><img src="https://img.shields.io/badge/python-3.7+-aff.svg"></a>
    <a href=""><img src="https://img.shields.io/badge/os-x86%2C%20aarch-pink.svg"></a>
    <a href="https://github.com/sophgo/LLM-TPU/graphs/contributors"><img src="https://img.shields.io/github/contributors/sophgo/LLM-TPU?color=9ea"></a>
    <a href="https://github.com/sophgo/LLM-TPU/issues"><img src="https://img.shields.io/github/issues/sophgo/LLM-TPU?color=9cc"></a>
    <a href="https://github.com/sophgo/LLM-TPU/commits"><img src="https://img.shields.io/github/commit-activity/y/sophgo/LLM-TPU?color=3af"></a>
</p>
<p align="center">
    <a href="https://github.com/sophgo/LLM-TPU/forks"><img src="https://img.shields.io/github/forks/sophgo/LLM-TPU?color=9cc"></a>
    <a href="https://github.com/sophgo/LLM-TPU/stargazers"><img src="https://img.shields.io/github/stars/sophgo/LLM-TPU?color=9cc"></a>
</p>

# ç›®å½•
  - [ä»‹ç»](#ä»‹ç»)
  - [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
  - [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
  - [èµ„æ–™é“¾æ¥](#èµ„æ–™é“¾æ¥)

# æœ€è¿‘æ›´æ–°ï¼ ğŸ”¥ğŸ”¥ğŸ”¥

- ğŸš€ **DeepSeekæ—¶åˆ»ï¼ï¼**: æˆ‘ä»¬é€‚é…äº† **DeepSeek-R1-Distill-Qwen-1.5B** å’Œ **DeepSeek-R1-Distill-Qwen-7B**çš„é€‚é…ï¼Œè¯¦æƒ…è§[language_model/python_demo](./models/language_model/python_demo/)ã€‚

# ä»‹ç»

æœ¬é¡¹ç›®å®ç°ç®—èƒ½BM1684XèŠ¯ç‰‡éƒ¨ç½²å„ç±»å¼€æº`ç”Ÿæˆå¼AIæ¨¡å‹`ï¼Œå…¶ä¸­ä»¥LLMä¸ºä¸»ã€‚é€šè¿‡[TPU-MLIR](https://github.com/sophgo/tpu-mlir)ç¼–è¯‘å™¨å°†æ¨¡å‹è½¬æ¢æˆbmodelï¼Œå¹¶é‡‡ç”¨c++ä»£ç å°†å…¶éƒ¨ç½²åˆ°PCIEç¯å¢ƒæˆ–è€…SoCç¯å¢ƒã€‚åœ¨çŸ¥ä¹ä¸Šå†™äº†ä¸€ç¯‡è§£è¯»ï¼Œä»¥`ChatGLM2-6B`ä¸ºä¾‹ï¼Œæ–¹ä¾¿å¤§å®¶ç†è§£æºç ï¼š[ChatGLM2æµç¨‹è§£æä¸TPU-MLIRéƒ¨ç½²](https://zhuanlan.zhihu.com/p/641975976)

## æ¨¡å‹ä»‹ç»
å·²éƒ¨ç½²è¿‡çš„æ¨¡å‹å¦‚ä¸‹ï¼ˆæŒ‰ç…§é¦–å­—æ¯é¡ºåºæ’åˆ—ï¼‰ï¼š

| Model                         | Huggingface Link                                                                 |
|-------------------------------|---------------------------------------------------------------------------------|
| Baichuan2-7B                  | [LINK](https://huggingface.co/baichuan-inc/Baichuan2-7B-Chat)                   |
| ChatGLM3-6B                   | [LINK](https://huggingface.co/THUDM/chatglm3-6b)                                |
| ChatGLM4-9B                   | [LINK](https://huggingface.co/THUDM/glm-4-9b-chat)                              |
| CodeFuse-7B                   | [LINK](https://huggingface.co/codefuse-ai/CodeFuse-DevOps-Model-7B-Chat)        |
| DeepSeek-6.7B                 | [LINK](https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-instruct)         |
| DeepSeek-R1-Distill-Qwen-1.5B | [LINK](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)        |
| DeepSeek-R1-Distill-Qwen-7B   | [LINK](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)          |
| Falcon-40B                    | [LINK](https://huggingface.co/tiiuae/falcon-40b)                                |
| Phi-3-mini-4k                 | [LINK](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/)                |
| Qwen-7B                       | [LINK](https://huggingface.co/Qwen/Qwen-7B-Chat)                                |
| Qwen-14B                      | [LINK](https://huggingface.co/Qwen/Qwen-14B-Chat)                               |
| Qwen-72B                      | [LINK](https://huggingface.co/Qwen/Qwen-72B-Chat)                               |
| Qwen1.5-0.5B                  | [LINK](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat)                           |
| Qwen1.5-1.8B                  | [LINK](https://huggingface.co/Qwen/Qwen1.5-1.8B-Chat)                           |
| Qwen1.5-7B                    | [LINK](https://huggingface.co/Qwen/Qwen1.5-7B-Chat)                             |
| Qwen2-7B                      | [LINK](https://huggingface.co/Qwen/Qwen2-7B-Chat)                               |
| Qwen2.5-7B                    | [LINK](https://huggingface.co/Qwen/Qwen2.5-7B-Chat)                             |
| Llama2-7B                     | [LINK](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)                    |
| Llama2-13B                    | [LINK](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf)                   |
| Llama3-8B                     | [LINK](https://huggingface.co/meta-llama/Meta-Llama-3-8B)                       |
| Llama3.1-8B                   | [LINK](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B)                     |
| LWM-Text-Chat                 | [LINK](https://huggingface.co/LargeWorldModel/LWM-Text-Chat-1M)                 |
| MiniCPM3-4B                   | [LINK](https://huggingface.co/openbmb/MiniCPM3-4B)                              |
| Mistral-7B-Instruct           | [LINK](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)               |
| Stable Diffusion              | [LINK](https://huggingface.co/runwayml/stable-diffusion-v1-5)                   |
| Stable Diffusion XL           | [LINK](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)         |
| WizardCoder-15B               | [LINK](https://huggingface.co/WizardLM/WizardCoder-15B-V1.0)                    |
| Yi-6B-chat                    | [LINK](https://huggingface.co/01-ai/Yi-6B-Chat)                                 |
| Yi-34B-chat                   | [LINK](https://huggingface.co/01-ai/Yi-34B-Chat)                                |
| Qwen-VL-Chat                  | [LINK](https://huggingface.co/Qwen/Qwen-VL-Chat)                                |
| Qwen2-VL-Chat                 | [LINK](https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct)                        |
| InternVL2-4B                  | [LINK](https://huggingface.co/OpenGVLab/InternVL2-4B)                           |
| InternVL2-2B                  | [LINK](https://huggingface.co/OpenGVLab/InternVL2-2B)                           |
| MiniCPM-V-2_6                 | [LINK](https://huggingface.co/openbmb/MiniCPM-V-2_6)                            |
| Llama3.2-Vision-11B           | [LINK](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct)         |
| Molmo-7B-D-0924               | [LINK](https://huggingface.co/allenai/Molmo-7B-D-0924)                          |


å¦‚æœæ‚¨æƒ³è¦çŸ¥é“è½¬æ¢ç»†èŠ‚å’Œæºç ï¼Œå¯ä»¥åˆ°æœ¬é¡¹ç›®[models](./models)å­ç›®å½•æŸ¥çœ‹å„ç±»æ¨¡å‹éƒ¨ç½²ç»†èŠ‚ã€‚

å¦‚æœæ‚¨å¯¹æˆ‘ä»¬çš„èŠ¯ç‰‡æ„Ÿå…´è¶£ï¼Œä¹Ÿå¯ä»¥é€šè¿‡å®˜ç½‘[SOPHGO](https://www.sophgo.com/)è”ç³»æˆ‘ä»¬ã€‚

# å¿«é€Ÿå¼€å§‹

å…‹éš†LLM-TPUé¡¹ç›®ï¼Œå¹¶æ‰§è¡Œrun.shè„šæœ¬
```shell
git clone https://github.com/sophgo/LLM-TPU.git
./run.sh --model llama2-7b
```

è¯¦ç»†è¯·å‚è€ƒ[Quick Start](./docs/Quick_Start.md)

### æ•ˆæœå›¾
è·‘é€šåæ•ˆæœå¦‚ä¸‹å›¾æ‰€ç¤º

![](./assets/qwen-7b.png)

### Command Table

ç›®å‰ç”¨äºæ¼”ç¤ºçš„æ¨¡å‹ï¼Œå…¨éƒ¨å‘½ä»¤å¦‚ä¸‹è¡¨æ‰€ç¤º

| Model           | SoC                                         | PCIE                                         |
| :-------------- | :------------------------------------------ | :------------------------------------------- |
| ChatGLM3-6B     | ./run.sh --model chatglm3-6b --arch soc     | ./run.sh --model chatglm3-6b --arch pcie     |
| Llama2-7B       | ./run.sh --model llama2-7b --arch soc       | ./run.sh --model llama2-7b   --arch pcie     |
| Llama3-7B       | ./run.sh --model llama3-7b --arch soc       | ./run.sh --model llama3-7b   --arch pcie     |
| Qwen-7B         | ./run.sh --model qwen-7b --arch soc         | ./run.sh --model qwen-7b     --arch pcie     |
| Qwen1.5-1.8B    | ./run.sh --model qwen1.5-1.8b --arch soc    | ./run.sh --model qwen1.5-1.8b  --arch pcie   |
| Qwen2.5-7B      |                     \                       | ./run.sh --model qwen2.5-7b  --arch pcie     |
| LWM-Text-Chat   | ./run.sh --model lwm-text-chat --arch soc   | ./run.sh --model lwm-text-chat  --arch pcie  |
| WizardCoder-15B | ./run.sh --model wizardcoder-15b --arch soc | ./run.sh --model wizardcoder-15b --arch pcie |
| InternVL2-4B    | ./run.sh --model internvl2-4b --arch soc    | ./run.sh --model internvl2-4b --arch pcie    |
| MiniCPM-V-2_6   | ./run.sh --model minicpmv2_6  --arch soc    | ./run.sh --model minicpmv2_6 --arch pcie     |
| Molmo-7B-D-0924 |                     \                       | ./run.sh --model molmo-7b --arch pcie        |

## è¿›é˜¶åŠŸèƒ½
è¿›é˜¶åŠŸèƒ½è¯´æ˜ï¼š

| åŠŸèƒ½        | ç›®å½•                                                                       | åŠŸèƒ½è¯´æ˜              |
| ----------- | -------------------------------------------------------------------------- | --------------------- |
| å¤šèŠ¯        | [ChatGLM3/parallel_demo](./models/ChatGLM3/parallel_demo)                   | æ”¯æŒChatGLM3 2èŠ¯      |
|             | [Llama2/demo_parallel](./models/Llama2/demo_parallel)                       | æ”¯æŒLlama2 4/6/8èŠ¯    |
|             | [Qwen/demo_parallel](./models/Qwen/demo_parallel)                           | æ”¯æŒQwen 4/6/8èŠ¯      |
|             | [Qwen1_5/demo_parallel](./models/Qwen1_5/demo_parallel)                     | æ”¯æŒQwen1_5 4/6/8èŠ¯   |
| æŠ•æœºé‡‡æ ·    | [Qwen/jacobi_demo](./models/Qwen/jacobi_demo)                               | LookaheadDecoding     |
|             | [Qwen1_5/speculative_sample_demo](./models/Qwen1_5/speculative_sample_demo) | æŠ•æœºé‡‡æ ·              |
| prefillå¤ç”¨ | [Qwen/prompt_cache_demo](./models/Qwen/prompt_cache_demo)                   | å…¬å…±åºåˆ—prefillå¤ç”¨   |
|             | [Qwen/share_cache_demo](./models/Qwen/share_cache_demo)                     | å…¬å…±åºåˆ—prefillå¤ç”¨   |
|             | [Qwen1_5/share_cache_demo](./models/Qwen1_5/share_cache_demo)               | å…¬å…±åºåˆ—prefillå¤ç”¨   |
| æ¨¡å‹åŠ å¯†    | [Qwen/share_cache_demo](./models/Qwen/share_cache_demo)                     | æ¨¡å‹åŠ å¯†              |
|             | [Qwen1_5/share_cache_demo](./models/Qwen1_5/share_cache_demo)               | æ¨¡å‹åŠ å¯†              |


# å¸¸è§é—®é¢˜

è¯·å‚è€ƒ[LLM-TPUå¸¸è§é—®é¢˜åŠè§£ç­”](./docs/FAQ.md)

# èµ„æ–™é“¾æ¥

* ChatGLM2æµç¨‹è§£æä¸TPU-MLIRéƒ¨ç½²ï¼šhttps://zhuanlan.zhihu.com/p/641975976
* æ¨¡å‹è½¬æ¢å·¥å…·é“¾ TPU-MLIRï¼šhttps://github.com/sophgo/tpu-mlir
* TPU-MLIRå¿«é€Ÿå…¥é—¨æ‰‹å†Œï¼šhttps://tpumlir.org/docs/quick_start/index.html
* TPU-MLIRè®ºæ–‡ã€æ•´ä½“å·¥ç¨‹è®²è§£ï¼šhttps://www.bilibili.com/video/BV1My4y1o73Q
